import os
import time
import boto3
import mysql.connector
from dotenv import load_dotenv

# Load the exact same .env file we used for Node.js
load_dotenv()

# Set up AWS S3 connection
s3 = boto3.client(
    's3',
    region_name=os.getenv('AWS_REGION'),
    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')
)
BUCKET_NAME = os.getenv('AWS_S3_BUCKET_NAME')

def connect_to_db():
    return mysql.connector.connect(
        host=os.getenv('DB_HOST'),
        user=os.getenv('DB_USER'),
        password=os.getenv('DB_PASSWORD'),
        database=os.getenv('DB_NAME')
    )

def process_jobs():
    print("Worker started. Waiting for jobs...")
    
    while True:
        try:
            db = connect_to_db()
            cursor = db.cursor(dictionary=True)
            
            # 1. Look for a job that is QUEUED
            cursor.execute("SELECT * FROM Jobs WHERE status = 'QUEUED' LIMIT 1")
            job = cursor.fetchone()
            
            if job:
                job_id = job['id']
                original_key = job['original_image_key']
                print(f"\n[+] Found Job: {job_id}")
                
                # 2. Mark as PROCESSING so no other worker grabs it
                cursor.execute("UPDATE Jobs SET status = 'PROCESSING' WHERE id = %s", (job_id,))
                db.commit()
                print("    -> Status updated to PROCESSING")
                
                # 3. Simulate downloading the image from S3
                local_image_path = f"temp_{job_id}.jpg"
                s3.download_file(BUCKET_NAME, original_key, local_image_path)
                print("    -> Image downloaded from S3")
                
                # 4. SIMULATE HEAVY AI WORK (Sleep for 5 seconds)
                print("    -> Simulating 2D to 3D AI generation...")
                time.sleep(5) 
                
                # Create a dummy 3D .obj file
                result_key = f"results/{job_id}-model.obj"
                local_result_path = f"temp_{job_id}.obj"
                with open(local_result_path, 'w') as f:
                    f.write("# Dummy 3D Object File generated by AsyncArt Worker\n")
                    f.write("v 0.0 0.0 0.0\n")
                
                # 5. Upload the "3D" result back to S3
                s3.upload_file(local_result_path, BUCKET_NAME, result_key)
                print("    -> 3D model uploaded to S3")
                
                # 6. Mark job as COMPLETED in the database
                cursor.execute(
                    "UPDATE Jobs SET status = 'COMPLETED', result_file_key = %s WHERE id = %s", 
                    (result_key, job_id)
                )
                db.commit()
                print("    -> Status updated to COMPLETED. Job done!")
                
                # Clean up local temporary files
                os.remove(local_image_path)
                os.remove(local_result_path)
                
            cursor.close()
            db.close()
            
        except Exception as e:
            print(f"Error checking jobs: {e}")
            
        # Wait 3 seconds before checking the database again
        time.sleep(3)

if __name__ == "__main__":
    process_jobs()